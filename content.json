{"meta":{"title":"老六","subtitle":"只要今天努力，明天会比昨天更美好","description":"君子所取者远，则必有所待，所就者大，则必有所忍，志大而量广，才有余而识具足，则必取其远，必就其大！！","author":"景宸","url":"http://yoursite.com"},"pages":[{"title":"","date":"2017-04-21T03:10:03.000Z","updated":"2017-04-21T03:10:03.000Z","comments":true,"path":"about/index.html","permalink":"http://yoursite.com/about/index.html","excerpt":"","text":"关于作者 我是老六，真名尹浩传。 2014年计算应用技术专业硕士毕业加入阿里，从事地图数据处理相关工作，包括竞品数据处理和基于高德地图大数据挖掘反补母库。 两年后加入好未来教育，主要从事ETL开发、数据监控系统、BI数据开发、大数据平台开发工作。 最熟悉的编程语言是Python，也会一点C++和Java。日常数据开发使用pyton，数据相关服务采用Django框架开发，数据存储采用MySQL数据库。 了解大数据开发，常用Hadoop streaming + Python进行MR开发，对hive、spark有一定的了解。 工作经历 阿里移动事业群-高德地图 好未来教育集团-工程研发中心-大数据部 教育背景 华中师范大学-计算机应用，工学硕士 武汉纺织大学-计算机科学与技术，工学学士"},{"title":"categories","date":"2016-11-11T08:00:09.000Z","updated":"2016-11-11T08:06:42.000Z","comments":true,"path":"categories/index.html","permalink":"http://yoursite.com/categories/index.html","excerpt":"","text":""},{"title":"comment","date":"2016-07-07T13:43:11.000Z","updated":"2017-09-23T14:23:28.000Z","comments":true,"path":"comment/index.html","permalink":"http://yoursite.com/comment/index.html","excerpt":"","text":"你好，欢迎来到老六的blog"},{"title":"tags","date":"2016-11-11T07:54:59.000Z","updated":"2016-11-11T08:08:49.000Z","comments":true,"path":"tags/index.html","permalink":"http://yoursite.com/tags/index.html","excerpt":"","text":""}],"posts":[{"title":"MySQL数据库优化","slug":"数据库优化","date":"2017-09-16T04:17:26.000Z","updated":"2017-09-16T04:24:52.000Z","comments":true,"path":"2017/09/16/数据库优化/","link":"","permalink":"http://yoursite.com/2017/09/16/数据库优化/","excerpt":"","text":"本文大部分内容摘自简书网 大牛的经验之谈：数据库查询速度优化技巧及解决方案， 后续会结合自己在工作中的经历做一些针对性的案例分析。 1，尽可能减少表的全局扫描 减少where字段值null判断 尽可能地将可能为null的字段设置一个可区分的默认值 尽量避免在where子句中使用!= 或&lt;&gt;操作符 尽量避免在where子句中使用or来连接条件 尽量避免使用in和not in 2，不要在条件判断时进行算数运算所以不要在 where 子句中的“=”左边进行函数、算术运算或其他表达式运算,这样系统将可能无法正确使用索引。 3，利用exists代替in4，索引技巧并不是所有索引对查询都有效 SQL是根据表中数据来进行查询优化的，当索引列有大量数据重复时，SQL查询可能不会去利用索引，如一表中有字段sex，male、female几乎各一半，那么即使在sex上建了索引也对查询效率起不了作用。 索引要有区分度。 索引并不是越多越好索引固然可以提高相应的 select 的效率，但同时也降低了 insert 及 update 的效率，因为 insert 或 update 时有可能会重建索引，所以怎样建索引需要慎重考虑，视具体情况而定。一个表的索引数最好不要超过6个，若太多则应考虑一些不常使用到的列上建的索引是否有必要。 应尽可能的避免更新 clustered 索引数据列因为 clustered 索引数据列的顺序就是表记录的物理存储顺序，一旦该列值改变将导致整个表记录的顺序的调整，会耗费相当大的资源。若应用系统需要频繁更新 clustered 索引数据列，那么需要考虑是否应将该索引建为 clustered 索引。 尽量使用数字型字段若只含数值信息的字段尽量不要设计为字符型，这会降低查询和连接的性能，并会增加存储开销。这是因为引擎在处理查询和连接时会逐个比较字符串中每一个字符，而对于数字型而言只需要比较一次就够了。 5，创建数据库时应该注意地方尽可能的使用 varchar/nvarchar 代替 char/nchar因为首先变长字段存储空间小，可以节省存储空间，其次对于查询来说，在一个相对较小的字段内搜索效率显然要高些。 用表变量来代替临时表。 如果表变量包含大量数据，请注意索引非常有限（只有主键索引）。 在新建临时表时，如果一次性插入数据量很大，那么可以使用 select into 代替 create table，避免造成大量 log ，以提高速度；如果数据量不大，为了缓和系统表的资源，应先create table，然后insert。 如果使用到了临时表，在存储过程的最后务必将所有的临时表显式删除，先 truncate table ，然后 drop table ，这样可以避免系统表的较长时间锁定。 避免频繁创建和删除临时表，以减少系统表资源的消耗。 6，尽量避免使用游标 因为游标的效率较差，如果游标操作的数据超过1万行，那么就应该考虑改写。 使用基于游标的方法或临时表方法之前，应先寻找基于集的解决方案来解决问题，基于集的方法通常更有效。 与临时表一样，游标并不是不可使用。对小型数据集使用 FAST_FORWARD 游标通常要优于其他逐行处理方法，尤其是在必须引用几个表才能获得所需的数据时。在结果集中包括“合计”的例程通常要比使用游标执行的速度快。如果开发时间允许，基于游标的方法和基于集的方法都可以尝试一下，看哪一种方法的效果更好。 7，数据放回时注意什么 尽量避免大事务操作，提高系统并发能力。这样可以有效提高系统的并发能力 尽量避免向客户端返回大数据量。若数据量过大，应该考虑相应需求是否合理。","categories":[{"name":"数据开发","slug":"数据开发","permalink":"http://yoursite.com/categories/数据开发/"}],"tags":[{"name":"数据库, MySQL,  后端","slug":"数据库-MySQL-后端","permalink":"http://yoursite.com/tags/数据库-MySQL-后端/"}]},{"title":"Python后端开发——大纲","slug":"Python后端开发——大纲","date":"2017-09-10T00:47:53.000Z","updated":"2017-09-10T00:50:38.000Z","comments":true,"path":"2017/09/10/Python后端开发——大纲/","link":"","permalink":"http://yoursite.com/2017/09/10/Python后端开发——大纲/","excerpt":"","text":"最近准备整理工作中用到的一些知识，先列一个大纲，作为这个系列的目录，争取每周至少一更。 Python基础语法常用数据结构 list dict set，frozenset tuple 常用语法 列表推导式 字典推导式 集合推导式 迭代器 生成器 装饰器 常用内置库 re os sys string collections itertools json datetime hashlib pickle ConfigParser 常用第三方库 requests beautifulsoup SQLalchmey pymysql 框架djangoflasktornadotwistedgeventRestful API数据库 MySQL Redis Mongodb Memcache PostgreSQL 优化","categories":[{"name":"数据开发","slug":"数据开发","permalink":"http://yoursite.com/categories/数据开发/"}],"tags":[{"name":"数据开发","slug":"数据开发","permalink":"http://yoursite.com/tags/数据开发/"}]},{"title":"数据开发规范-补充中","slug":"数据开发规范-补充中","date":"2017-09-08T09:52:18.000Z","updated":"2017-09-08T10:22:44.000Z","comments":true,"path":"2017/09/08/数据开发规范-补充中/","link":"","permalink":"http://yoursite.com/2017/09/08/数据开发规范-补充中/","excerpt":"","text":"数据库相关字段命名字段命名应该结合业务需求，应注意如下： 可读性，通过名称就能知晓该字段意思，但不宜过长； 从长期的接口开发工作总结知晓，数据分为以下几种： 数值型：此类数据，不考虑小数点后数据 小数型：此类数据，需考虑小数点后保留位数 百分数型：此类数据命名时应加为rate或ratio作为标识，与前端约定此类数据均按百分比展示，即末尾添加‘%’符号 趋势数据一般按照时间字段来展示，所以在建表时，应将最终需要展示的时间维度数据以展示结果格式来创建，若该字段无法准确排序，应添加时间维度排序编号 维度数据不宜过长，且应添加排序编号 索引创建，应将所有维度都创建索引，方便在接口操作的时候快速定位数据。 数据结果普查很多时候，产品经理在画原型图时没有接触实际的业务数据，无法根据实际情况考虑最终展示结果，所以数据开发人员在处理完结果数据并入库后应查看结果数据表，并对数据做排查，review所有的情况，做到心中有数。建议从以下方面来排查： 维度数据齐全，与最终展示相符； 维度映射关系明确，做到一对一映射； 所有数值合理，以下范例可供参考： 百分数范围在(0,1)，如若遇特殊情况超出，应做说明； 司龄数值合理，之前有发现该数据存在260，明显是不合理，应排查处理逻辑； 数据接口开发因为数据组一般接口开发都采用django框架，所以本文档仅针对数据组内部情况说明，分为API形式和非API形式。 API形式此处不做过多说明，直接调用rest_framework框架处理即可。 非API形式非API形式，主要是为了前端开发人员节省时间。以非API形式提供接口，主要是方便在多个筛选的情况下快速开发接口。 从最终展示来看，数据接口大概分为以下几种： 表格：此类数据可处理为header + content的形式 header：列出表头展示内容和对应content中的数据字段名； content：按照表头排序输出列表 趋势图：维度信息和数值内容 维度信息：一般是多维度。其中时间排序即可。 数值信息：一般为数据，定义统一处理函数对数据进行格式化。 柱形图：与趋势图类似 联动选项：主要是针对维度筛选，此处定义多层字典即可。","categories":[{"name":"数据开发","slug":"数据开发","permalink":"http://yoursite.com/categories/数据开发/"}],"tags":[{"name":"数据开发","slug":"数据开发","permalink":"http://yoursite.com/tags/数据开发/"}]},{"title":"mysql数据库权限操作","slug":"mysql数据库权限操作","date":"2016-12-06T09:05:31.000Z","updated":"2017-09-16T04:24:16.000Z","comments":true,"path":"2016/12/06/mysql数据库权限操作/","link":"","permalink":"http://yoursite.com/2016/12/06/mysql数据库权限操作/","excerpt":"","text":"设置用户权限以root账号登录mysql mysql -hlocalhost -uroot -p mysql&gt; use mysql; 创建用户create user mike identified by &apos;mike&apos;; create user rainbow@localhost identified by &apos;rainbow&apos;; 修改用户名rename user mike to john; 删除用户drop user rainbow; 查看用户select host,user from user; 更改密码update mysql.user set authentication_string=password(&apos;123qwe&apos;) where user=&apos;john&apos;; alter user &apos;john&apos;@&apos;localhost&apos; identified by &apos;123qwe&apos;; 给用户添加权限常规权限： select，查 update，改 delete，删 insert，增 数据库开发人员权限： create, 建表 alter, 修改 drop, 删除 references, 操作外键权限 create tempoary tables, 操作临时表权限 index, 操作索引权限 create view, 操作试图权限 show view, 查看试图源代码权限 create routine, 操作存储过程权限 alter routine, execute, 操作mysq函数权限 DBA权限： grant all on *.* to dba@&apos;localhost&apos; #高级DBA管理MySQL中所有数据库权限 设置权限时必须给出一下信息 要授予的权限 被授予访问权限的数据库或表 用户名 grant和revoke可以在几个层次上控制访问权限 整个服务器，使用 grant ALL 和revoke ALL 整个数据库，使用on database.* 特点表，使用on database.table 特定的列 特定的存储过程 user表中host列的值的意义 % 匹配所有主机 localhost localhost不会被解析成IP地址，直接通过UNIXsocket连接 127.0.0.1 会通过TCP/IP协议连接，并且只能在本机访问 ::1 ::1就是兼容支持ipv6的，表示同ipv4的127.0.0.1 1.为用户adbibo添加数据库test中所有表的查询权限 grant select on test.* to &apos;adbibo&apos;@&apos;localhost&apos; identified by &apos;adbibo&apos;; 2.为用户adbibo添加数据库test中所有表的增删改权限 grant update,delete,insert on test.* to &apos;adbibo&apos;@&apos;localhost&apos; identified by &apos;adbibo&apos;; 3.为用户adbibo添加数据库中所有的权限，最高权限 grant all privileges on *.* to &apos;adbibo&apos;@&apos;localhost&apos; identified by &apos;adbibo&apos;; 4.删除用户adbibo插入数据库的权限 revoke insert on *.* from &apos;adbibo&apos;@&apos;localhost&apos;; 附： 权限表 权限 说明 all alter 修改表 create 创建表 drop 删除表 index 可以使用create index 和drop index create view 创建试图 show view 显示视图内容 select 查询表 update 更新表内容 delete 删除表内容 insert 向表中插入记录 grant option 可以使用grant create user 创建用户 show databases 显示当前用户可访问的所有数据库名 alter routine 使用alter procedure 和drop procedure create routine 使用create procedure create temporary tables 使用create temporary table execute 使用call和存储过程 file 使用select into outfile 和load data infile和revoke lock tables 锁表 process 使用show full processlist reload 使用flush replication client 服务器位置访问 replocation slave 由复制从属使用 shutdown 使用mysqladmin shutdown 来关闭mysql super usage 无访问权限","categories":[{"name":"数据开发","slug":"数据开发","permalink":"http://yoursite.com/categories/数据开发/"}],"tags":[{"name":"数据库, MySQL","slug":"数据库-MySQL","permalink":"http://yoursite.com/tags/数据库-MySQL/"}]},{"title":"GIT使用实战","slug":"GIT使用实战","date":"2016-11-11T07:48:49.000Z","updated":"2016-11-11T08:33:38.000Z","comments":true,"path":"2016/11/11/GIT使用实战/","link":"","permalink":"http://yoursite.com/2016/11/11/GIT使用实战/","excerpt":"","text":"参考教程推荐廖雪峰的Git教程 省去账号申请等信息，以我自己使用过程遇到的问题和解决的方式来给大家讲讲。 问题一：实际coding时，我有github平台和gitlab平台的需求，如何在本机配置？一般.ssh都在用户目录下， # adbibo @ bogon in ~/.ssh [15:04:49] $ pwd /Users/adbibo/.ssh 生成一个新的ssh key $ ssh-keygen -t rsa -b 4096 -C &quot;your_email@example.com&quot; Generating public/private rsa key pair. Enter file in which to save the key (/Users/adbibo/.ssh/id_rsa):/Users/adbibo/.ssh/test_id_rsa #在此输入你要保存ssh key的实际文件名，为了区分github和gitlab，请将ssh key存储在不同的文件中。 Enter passphrase (empty for no passphrase): 输入密码，鄙人比较懒，直接回车跳过 Enter same passphrase again:同上 Your identification has been saved in /Users/adbibo/.ssh/test_id_rsa. Your public key has been saved in /Users/adbibo/.ssh/test_id_rsa.pub. The key fingerprint is: SHA256:xRAuPVjXDjJUKihSqQWb9TwBMH3Be4ChaMxm8C+3j24 your_email@example.com The key&apos;s randomart image is: +---[RSA 4096]----+ |=+=*o. .=oo. | |=O=o+o =o=. . | |*Oo.=o+ =ooo | |=. o...o o . | | . o. S | | o . | | . | | Eo | | oo . | +----[SHA256]-----+ 生成的ssh key分别将验证信息和公钥存储在test_id_rsa和test_id_rsa.pub文件中。 在.ssh目录下创建文件config $ cd ~/.ssh $ touch config 添加如下内容： # gitlab Host gitlab.com HostName gitlab.com IdentityFile ~/.ssh/gitlab_rsa # github Host github.com HostName github.com IdentityFile ~/.ssh/github_rsa 至此，gitlab和github的项目通过git命令管理了。 问题二：如果将本地的工程提交到远程仓库？这里以github为例， 首先在github.com上创建工程，点击”New repository”，输入Repository name，填写Description，选择项目是public还是private，然后点击”Create repository”。得到一个如下形式的git仓库名。 git@github.com:yourgithubname/test.git 然后回到本机，在工程目录下执行如下命令： git init # 在本地初始化git仓库 git add . # 添加当前目录下的所有文件作为需要版本管理的文件 git commit -m &quot;第一次提交&quot; # 填写提交信息 git remote add origin git@github.com:yourgithubname/test.git # 关联远程仓库 git push -u origin master # 将本地文件提交到远程仓库 一般都会顺利地进行至此，但是我在github.com上创建工程时，选择了生成.gitignore文件，于是远程仓库初始化时就有了文件。再在本机进行上面的操作就会报错了。 To github.com:yourgithubname/test.git ! [rejected] master -&gt; master (fetch first) error: failed to push some refs to &apos;git@github.com:yourgithubname/test.git&apos; hint: Updates were rejected because the remote contains work that you do hint: not have locally. This is usually caused by another repository pushing hint: to the same ref. You may want to first integrate the remote changes hint: (e.g., &apos;git pull ...&apos;) before pushing again. hint: See the &apos;Note about fast-forwards&apos; in &apos;git push --help&apos; for details. 提示需要先git pull，就是先要将远程仓库的文件pull到本机，但是由于本机使用IDE自动生成了.gitignore文件，与远程仓库发生了冲突。需要带上参数–allow-unrelated-histories $ git pull origin master --allow-unrelated-histories From github.com:yourgithubname/test * branch master -&gt; FETCH_HEAD Auto-merging README.md CONFLICT (add/add): Merge conflict in README.md Auto-merging .gitignore CONFLICT (add/add): Merge conflict in .gitignore Automatic merge failed; fix conflicts and then commit the result. 此时又提示merge conflict， 所以需要手动做更改.gitignore和README.md文件，具体根据需求进行更改。然后重新执行如下命令： $git add . $git commit -m &apos;commot message&apos; $ git push 显示如下信息就说明commit成功了。 Counting objects: 17, done. Delta compression using up to 4 threads. Compressing objects: 100% (15/15), done. Writing objects: 100% (17/17), 4.15 KiB | 0 bytes/s, done. Total 17 (delta 2), reused 0 (delta 0) 问题三：遇到再补充。常用git仓库 github.com 开源项目很多，可以管理个人创建的项目，public的项目免费，如果创建private的需要money。可以搭个人blog，我用的hexo，简单配置即可。 gitlab.com 也不错，private项目免费，看中的就这点，毕竟github用的很习惯。 对于不喜欢命令行的朋友，推荐使用sourceTree这个工具来管理， 附上链接sourceTree","categories":[{"name":"其他","slug":"其他","permalink":"http://yoursite.com/categories/其他/"}],"tags":[{"name":"GIT","slug":"GIT","permalink":"http://yoursite.com/tags/GIT/"}]},{"title":"使用Cloudera快速部署Hadoop集群(二)","slug":"使用Cloudera快速部署Hadoop集群-二","date":"2016-09-07T03:36:12.000Z","updated":"2016-11-11T08:33:44.000Z","comments":true,"path":"2016/09/07/使用Cloudera快速部署Hadoop集群-二/","link":"","permalink":"http://yoursite.com/2016/09/07/使用Cloudera快速部署Hadoop集群-二/","excerpt":"","text":"一,Cloudera Manager 安装后的各个目录 /var/log/cloudera-scm-installer : 安装日志目录。 /var/log/* : 相关日志文件（相关服务的及CM的）。 /usr/share/cmf/ : 程序安装目录。 /usr/lib64/cmf/ : Agent程序代码。 /var/lib/cloudera-scm-server-db/data: 内嵌数据库目录。 /usr/bin/postgres : 内嵌数据库程序。 /etc/cloudera-scm-agent/ : agent的配置目录。 /etc/cloudera-scm-server/ : server的配置目录。 /opt/cloudera/parcels/ : Hadoop相关服务安装目录。 /opt/cloudera/parcel-repo/ : 下载的服务软件包数据，数据格式为parcels。 /opt/cloudera/parcel-cache/ : 下载的服务软件包缓存数据。 /etc/hadoop/* : 客户端配置文件目录。 二,各节点服务详述利用Cloudera Manager安装CDH，需要配置各节点角色，现将各服务节点角色一一说明。 1,HDFSApache hadoop官网描述如下： Hadoop Distributed File System (HDFS™): A distributed file system that provides high-throughput access to application data. Gateway HttpFS NameNode DataNode SecondaryNameNode Balancer HttpFS NFS Gateway 2,HBase Gateway Master HBase REST Server HBase Thrift Server Region Server 3,YRAN Resource Manager JobHistory Server NodeManager 4,Spark History Server Gateway Livy Server 5,ZooKeeper server 6,Hive Gateway Hive Metastore Server WebHcat HiveServer2 7,HUE Hue Server 8,Impala Impala Catalog Server Impala Llama ApplicationMaster Impala Daemon 9,Oozie Oozie Server 10,Solr Gateway Solr Server 11,Key-Value Store Indexer Lily HBase Indexer","categories":[{"name":"平台开发","slug":"平台开发","permalink":"http://yoursite.com/categories/平台开发/"}],"tags":[{"name":"平台开发","slug":"平台开发","permalink":"http://yoursite.com/tags/平台开发/"},{"name":"CDH","slug":"CDH","permalink":"http://yoursite.com/tags/CDH/"}]},{"title":"使用Cloudera快速部署Hadoop集群(一)","slug":"使用Cloudera快速部署Hadoop集群-一","date":"2016-08-29T14:10:44.000Z","updated":"2016-11-11T08:33:42.000Z","comments":true,"path":"2016/08/29/使用Cloudera快速部署Hadoop集群-一/","link":"","permalink":"http://yoursite.com/2016/08/29/使用Cloudera快速部署Hadoop集群-一/","excerpt":"","text":"换到新公司已经一个半月，做了两个小项目，现在在进行第三个大项目。由于部门是新成立的，需要搭建数据中心的基础数据平台，我先试个水。 之前的工作经历导致对Hadoop生态了解的不多，因为原来公司的数据都在阿里云上。不需要了解细节就可以很好地工作，而那时的我也比较懵逼，所以。。。 好久没有更新blog，准备接下来每周都讲讲自己平台搭建的工作。所有文中提到的参考链接均为查阅资料时的参考。 Cloudera Manager安装1，Cloudera Manager安装包： http://archive.cloudera.com/cm5/redhat/6/x86_64/cm/5/RPMS/x86_64/ 2，Cloudera Manager资源http://archive.cloudera.com/cm5/cm/5/ 3，parcelshttp://archive.cloudera.com/cdh5/parcels/5.8/ 4，tarballhttp://archive.cloudera.com/cdh5/repo-as-tarball/5.8.0/ CDH服务配置1，解析Cloudera Manager内部结构、功能包括配置文件、目录位置等http://my.oschina.net/cloudcoder/blog/362374 2，YARN/MRv2 Resource Manager深入剖析—RM总体架构http://dongxicheng.org/mapreduce-nextgen/yarnmrv2-resource-manager-infrastructure/ 3，HDFS详解http://my.oschina.net/crxy/blog/348868 http://blog.chinaunix.net/uid-27105712-id-3274395.html 4，HUEhttp://ju.outofmemory.cn/entry/105162 5，Hive详细教程http://my.oschina.net/yangzhiyuan/blog/228362 6，HBasehttp://www.cnblogs.com/JemBai/archive/2012/07/21/2602432.html","categories":[{"name":"平台开发","slug":"平台开发","permalink":"http://yoursite.com/categories/平台开发/"}],"tags":[{"name":"平台开发","slug":"平台开发","permalink":"http://yoursite.com/tags/平台开发/"},{"name":"CDH","slug":"CDH","permalink":"http://yoursite.com/tags/CDH/"}]},{"title":"认识ETL","slug":"认识ETL","date":"2016-07-23T13:37:41.000Z","updated":"2016-11-11T08:33:35.000Z","comments":true,"path":"2016/07/23/认识ETL/","link":"","permalink":"http://yoursite.com/2016/07/23/认识ETL/","excerpt":"","text":"最近两年一直从事数据处理相关的工作，一年前开始接触数据挖掘的工作，第一个项目就是将两个不同数据源的数据导入项目组odps应用。从数据存放的角度，我的工作分为三部分：1，数据下载（数据存放在不同的服务器）；2，数据清洗格式统一（需要与内部的其它数据保持一致的格式）；3，数据同步至odps（开篇提到的数据需要导入至odps应用中）。其中，主要工作在第二步。当时没有ETL的概念，直到最近新入职一家公司的数据部门，花了不到一周的时间重构了一套ETL流程，才意识到自己一直在从事自己并不认为是ETL开发的工作。于是决定趁今天晚上的时间，利用网络上已有的知识系统地学习ETL。 什么是ETL？ETL－百度百科 ETL，是英文 Extract-Transform-Load 的缩写，用来描述将数据从来源端经过抽取（extract）、转换（transform）、加载（load）至目的端的过程。ETL一词较常用在数据仓库，但其对象并不限于数据仓库。ETL是构建数据仓库的重要一环，用户从数据源抽取出所需的数据，经过数据清洗,最终按照预先定义好的数据仓库模型，将数据加载到数据仓库中去。 以下内容转自MichaelLau的博客 E－extract需要解决的问题包括： a、数据的时间差异性问题 在抽取旧有数据时，要将不同时期的数据定义统一，较早的数据不够完整或不符合新系统的数据规范，一般可以根据规则，在存入中转区的过程中予以更新或补充。 b、数据的平台多样性问题 在抽取旧有数据时，大部分数据都可采用表复制方式直接导入数据中转区集中，再做处理，但有部分数据可能需要转换成文本文件或使用第三方工具如 Informatica等装载入数据中转区。这部分数据主要是与数据中转区数据库平台不一致的数据库数据，或非存储于数据库内的文本、excel等数据。 c 、数据的不稳定性问题 对于重要信息的完整历史变更记录，在抽取时可以根据各时期的历史信息，在抽取需要信息等基本属性的旧有数据时，要与相应时段的信息关联得到真实的历史属性。 d 、数据的依赖性问题 旧有业务系统的数据关联一般已有约束保证，代码表和参照表等数据也比较准确，但仍有少量数据不完整，对这部分数据，需根据地税的需求采取清洗策略，保证数据仓库各事实表和维表之间的关联完整有效。 数据仓库各事实表和维表的初始装载顺序有先后关系，要有一个集中的数据装载任务顺序方案，确保初始数据装载的准确。这可以通过操作系统或第三方工具的任务调度机制来保证。 T-transform需要注意的问题 数据清洗主要是针对源数据库中出现二义性、重 复、不完整、违反业务或逻辑规则等问题的数据数据进行统一的处理，一般包括如：NULL值处理，日期格式转换，数据类型转换等等。在清洗之前需要进行数据 质量分析，以找出存在问题的数据，否则数据清洗将无从谈起。数据装载是通过装载工具或自行编写的SQL程序将抽取、转换后的结果数据加载到目标数据库中。 数据质量问题具体表现在以下几个方面： a、正确性（Accuracy）：数据是否正确的表示了现实或可证实的来源? b、完整性（Integrity）：数据之间的参照完整性是否存在或一致? c、一致性（Consistency）：数据是否被一致的定义或理解? d、完备性（Completeness）：所有需要的数据都存在吗? e、有效性（Validity）：数据是否在企业定义的可接受的范围之内? f、时效性（Timeliness）：数据在需要的时侯是有效的吗? g、可获取性（Accessibility）：数据是否易于获取、易于理解和易于使用? 以下综合说明数据仓库中数据质量要求，包括格式、完整性要求。 a、业务描述统一，对数据模型的不同版本融合、映射为唯一版本。包括： 1、在业务逻辑没有变化的前提下，旧的业务数据映射在新模型上。 2、 遗留系统的人事信息、考核相关信息与业务系统、行政其他模块要一致。 b、信息描述规范、完整。 1、不存在格式违规,数据类型不存在潜在错误。 2 、参照完整性未被破坏,数据不会找不到参照。 3 、不存在交叉系统匹配违规，数据被很好集成,相同的数据存在于多个系统中，数据之间要匹配。 4 、数据在内部一致,同样的纪录字段在同一个表中重复出现，不能有差别。 L-loading 将转换和清洗完的数据按照数据仓库的结构进行数据加载。需要考虑初始数据装载、数据刷新、加载顺序等等问题。 a、针对数据现状，初始导入有这样一些问题需要考虑： 1、如何解决时间差异性？ 2、如何解决平台差异性？ 3、如何适应数据的不稳定性？ 4、如何解决数据依赖性？ b、数据刷新的策略要根据业务需求和应用系统的承受能力和数据情况决定。主要有这样一些问题需要考虑： 1、如何解决时间差异性？ 2、如何适应数据的不稳定性？ 3、如何解决平台差异性？ 4、如何解决数据依赖性？ 5、如何减少对业务系统的影响？ c、不同的刷新任务类型，对业务系统的影响不同，刷新任务有以下种归类特性： 1、刷新频率：实时刷新、每数小时、每日、每周、每月、不定期手动刷新。 2、刷新方式：数据库表复制、文本文件ftp再装载、物化视图、数据库trigger。 3、数据加工方式：简单插入更新、增加计算项字段、多表关联更新、汇总、多表关联汇总计算。 并可针对各种异常情况做处理：回滚，重新装载，断点重新装载等等，还可在任务完成后（或失败后）将日志以Email方式发给数据仓库管理人员。 以上内容属于搬运，后续在项目中学习到的知识，再来进行补充。","categories":[{"name":"数据开发","slug":"数据开发","permalink":"http://yoursite.com/categories/数据开发/"}],"tags":[{"name":"数据开发","slug":"数据开发","permalink":"http://yoursite.com/tags/数据开发/"},{"name":"ETL","slug":"ETL","permalink":"http://yoursite.com/tags/ETL/"}]},{"title":"R语言入门-我的HelloWorld","slug":"R语言入门-我的HelloWorld","date":"2016-05-20T06:17:53.000Z","updated":"2016-11-11T08:33:43.000Z","comments":true,"path":"2016/05/20/R语言入门-我的HelloWorld/","link":"","permalink":"http://yoursite.com/2016/05/20/R语言入门-我的HelloWorld/","excerpt":"","text":"前几天，主管拿到一份数据，让我做一个分城市统计，原本想先对数据做一些清洗，然后在excel中做一些统计分析。但是转念一想，全国300+的城市，展示是个难题，想到前几天看到组里面另外一个同事——就是我之前提到过的追兵同学，他之前做过一个数据展示的界面，还不错，于是要来他的源码，一知半解地边查资料、边咨询，最后做了一个统计结果的展示界面了，也算是我的R语言入门的HelloWorld了。下面开始我将详细介绍实现R语言代码，其中涉及到公司内部的数据、规格的名称、名词全部更替，最终的也不会放上任何展示界面。 1，数据预处理拿到需要处理的数据，首先需要做预处理，这个切记！ 预处理时，需要根据自己的需求--最终展示需要的事项来展开。这里就不赘述了。 2，Ｒ语言展示部分### 先上代码。 #画图 library(ggplot2) library(plotly) #行列处理 library(dplyr) library(tidyr) #页面展示 library(shiny) #设置工作空间 setwd(&apos;/home/user/project-dir/data/&apos;) #读数据 #假设数据有城市（cityname）,数据类型（data_type）,数据等级（data_class）,其他字段（这里忽略）等字段 log_df &lt;- read.table(&apos;data_file&apos;, header = TRUE, sep = &quot;,&quot;, fileEncoding=&quot;utf-8&quot;) #两个文件有关联信息，这部分数据包含城市（cityname）,城市类型（citytype） config_df &lt;- read.table(&apos;config_file&apos;,header = TRUE, sep = &quot;,&quot;, fileEncoding = &quot;utf-8&quot;) #按城市、数据类型、等级聚合，统计 datatype_df &lt;- df_feedback %&gt;% group_by(cityname, data_type, data_class) %&gt;% summarise(plot_dt_cnt = n()) #按城市和数据类型聚合，统计 datatype_sum_df &lt;- df_feedback %&gt;% group_by(cityname, data_type) %&gt;% summarise(plot_dt_cnt_sum=n()) #聚合上述量表，主要是为了拿到各城市各类型的占比统计所需数据 datatype_df_together &lt;- dplyr::inner_join(datatype_df, datatype_sum_df, by=c(&quot;cityname&quot;, &quot;data_type&quot;)) #新增占比统计字段 datatype_df_together &lt;- datatype_df_together %&gt;% mutate(plot_dt_ratio=plot_dt_cnt/plot_dt_cnt_sum) #按城市和data_class计数 data_class_df &lt;- df_feedback %&gt;% group_by(cityname, data_class, data_type) %&gt;% summarise(plot_rc_cnt=n()) #界面 ui &lt;- fluidPage( #大标题 titlePanel(&quot;全国问题反馈数据相关分布&quot;), fluidRow( #小标题 h4(&quot;各种问题分布&quot;), #选项 column(3,selectInput(inputId = &quot;data_type&quot;, label=&quot;请选择需要展示的问题类型&quot;, choices=levels(datatype_df$data_type), multiple = TRUE, selected=&quot;数据类型-1&quot;)), #内容展示 column(9, plotlyOutput(outputId=&quot;plot_datatype_output&quot;)) ), fluidRow( #小标题 h4(&quot;各城市数据等级分布&quot;), #选项 column(3,selectInput(inputId = &quot;cityname&quot;, label=&quot;请选择需要展示的城市&quot;, choices=levels(data_class_df$cityname), multiple = TRUE, selected=&quot;北京市&quot;)), #内容展示 column(9, plotlyOutput(outputId=&quot;plot_cityname_output&quot;)) ), fluidRow( #小标题 h4(&quot;各城市数据等级分布&quot;), #选项1 column(2,selectInput(inputId = &quot;level&quot;, label=&quot;请选择需要的城市级别&quot;, choices=levels(citylevel_df$level), selected=&quot;S级&quot;)), #选项2 column(2,selectInput(inputId = &quot;level_cityname&quot;, label=&quot;请选择需要的城市&quot;, choices=levels(filter(citylevel_df, level==&quot;S级&quot;)[,1]), selected=&quot;北京市&quot;)), #内容展示 column(8, plotlyOutput(outputId=&quot;plot_level_cityname_output&quot;)) ) ) s_citylist &lt;- filter(citylevel_df, level==&quot;S级&quot;) [,1] #服务 server &lt;- function( input, output, session) { #按数据类型统计输出S级城市统计结果 output$plot_datatype_output &lt;- renderPlotly({ p_datatype &lt;- ggplot(datatype_df_together %&gt;% filter(data_type %in% input$data_type, cityname %in% s_citylist)) + geom_bar(aes(x=as.factor(cityname), weight=plot_dt_ratio, fill=as.factor(data_class)), position=&quot;dodge&quot;) + facet_wrap(~data_type) p_datatype &lt;- p_datatype + xlab(&quot;city&quot;) + ylab(&quot;percent&quot;) ggplotly(p_datatype) }) #按城市统计所有的data_class output$plot_cityname_output &lt;- renderPlotly({ p_cityname &lt;- ggplot(data_class_df %&gt;% filter(cityname %in% input$cityname)) + geom_bar(aes(x = as.factor(data_class), weight=plot_rc_cnt, fill= as.factor(data_type)), position = &quot;dodge&quot;) + xlab(&quot;data_class&quot;) + ylab(&quot;CNT&quot;) + facet_wrap(~cityname) ggplotly(p_cityname) }) #选项联动 choiced_citylist &lt;- reactive({ print(&quot;debug&quot;) citylist &lt;- filter(citylevel_df, level == input$level) %&gt;% mutate(cityname=as.character(cityname)) %&gt;% select(cityname) citylist &lt;- as.vector(citylist) updateSelectInput(session, &quot;level_cityname&quot;, choices = citylist) return (citylist) }) #按城市级别选择城市输出统计结果 output$plot_level_cityname_output &lt;- renderPlotly({ tmp &lt;- choiced_citylist() p_level_cityname &lt;- ggplot(data_class_df %&gt;% filter(cityname==input$level_cityname)) + geom_bar(aes(x = as.factor(data_class), weight=plot_rc_cnt, fill= as.factor(data_type)), position = &quot;dodge&quot;) + xlab(&quot;data_class&quot;) + ylab(&quot;CNT&quot;) + facet_wrap(~cityname) ggplotly(p_level_cityname) }) } shinyApp(ui=ui, server=server) 附：需要一台部署了shiny server的服务器。","categories":[{"name":"数据分析","slug":"数据分析","permalink":"http://yoursite.com/categories/数据分析/"}],"tags":[{"name":"R语言","slug":"R语言","permalink":"http://yoursite.com/tags/R语言/"}]},{"title":"如何配置fastCGI使nginx支持CGI","slug":"如何配置fastCGI使nginx支持CGI","date":"2016-05-12T02:28:47.000Z","updated":"2016-11-11T08:33:36.000Z","comments":true,"path":"2016/05/12/如何配置fastCGI使nginx支持CGI/","link":"","permalink":"http://yoursite.com/2016/05/12/如何配置fastCGI使nginx支持CGI/","excerpt":"","text":"之前组内有春日学习计划，安排我分享CGI相关的知识。于是就学习起来，整理了如下文档。 1，安装fcgi-spwan下载链接http://download.lighttpd.net/spawn-fcgi/releases-1.6.x/spawn-fcgi-1.6.2.tar.gz 安装：$wget http://download.lighttpd.net/spawn-fcgi/releases-1.6.x/spawn-fcgi-1.6.2.tar.gz $ tar -zxvf spawn-fcgi-1.6.2.tar.gz $ cd spawn-fcgi-1.6.2 $./configure $make $make install /usr/local/bin/ spawn-fcgi 默认路径 $spawn-fcgi –h Usage: spawn-fcgi [options] [-- &lt;fcgiapp&gt; [fcgi app arguments]] spawn-fcgi v1.6.2 (ipv6) - spawns FastCGI processes Options: -f &lt;path&gt; filename of the fcgi-application (ignored if &lt;fcgiapp&gt; is given) -d &lt;directory&gt; chdir to directory before spawning -a &lt;address&gt; bind to IPv4/IPv6 address (defaults to 0.0.0.0) -p &lt;port&gt; bind to TCP-port -s &lt;path&gt; bind to Unix domain socket -M &lt;mode&gt; change Unix domain socket mode -C &lt;children&gt; (PHP only) numbers of childs to spawn (default: not setting the PHP_FCGI_CHILDREN environment variable - PHP defaults to 0) -F &lt;children&gt; number of children to fork (default 1) -P &lt;path&gt; name of PID-file for spawned process (ignored in no-fork mode) -n no fork (for daemontools) -v show version -?, -h show this help (root only) -c &lt;directory&gt; chroot to directory -S create socket before chroot() (default is to create the socket in the chroot) -u &lt;user&gt; change to user-id -g &lt;group&gt; change to group-id (default: primary group of user if -u is given) -U &lt;user&gt; change Unix domain socket owner to user-id -G &lt;group&gt; change Unix domain socket group to group-id 2，安装fcgiwrap下载链接https://github.com/gnosek/fcgiwrap/archive/master.zip 安装：$wget https://github.com/gnosek/fcgiwrap/archive/master.zip $unzip master.zip $cd fcgiwrap-master/ $autoreconf –i $./configure 如果configure失败，一般都会提示”FastCGI libaray is missing”，确实fcgi-devel 缺少 $ wget http://dl.fedoraproject.org/pub/epel/5/x86_64/epel-release-5-4.noarch.rpm $ sudo rpm -Uvh epel-release*rpm $ sudo yum install fcgi-devel –y $ make $ make install /usr/local/sbin/ fcgiwrap 默认路径 Usage: fcgiwrap [OPTION] Invokes CGI scripts as FCGI. fcgiwrap version 1.1.0 Options are: -f Send CGI&apos;s stderr over FastCGI -c &lt;number&gt; Number of processes to prefork -s &lt;socket_url&gt; Socket to bind to (say -s help for help) -h Show this help message and exit -p &lt;path&gt; Restrict execution to this script. (repeated options will be merged) Report bugs to Grzegorz Nosek &lt;root@localdomain.pl&gt;. fcgiwrap home page: &lt;http://nginx.localdomain.pl/wiki/FcgiWrap&gt; 3，nginx安装下载链接http://tengine.taobao.org/download/tengine-2.1.2.tar.gz 安装 $wget http://tengine.taobao.org/download/tengine-2.1.2.tar.gz $ tar -zxvf tengine-2.1.2.tar.gz $ cd tengine-2.1.2 $./configure $make $make install /usr/local/nginx 默认路径 4，编写CGI脚本设置CGI脚本存放目录，这里只以shell为例，为了逼格，将.sh改为了.cgi $mkdir /usr/local/nginx/cgi-bin/ $cd /usr/local/nginx/cgi-bin/ $vim hello.cgi 1 #!/bin/bash 2 echo &quot;Content-Type:text/html&quot; 3 echo &quot;&quot; 4 5 date 6 echo -e &quot;\\nhello world!&quot; $sudo chmod 755 hello.cgi 5，启动命令sudo /usr/local/bin/spawn-fcgi -f /usr/local/sbin/fcgiwrap -a 127.0.0.1 -p 8092 -F 32 -P /usr/local/nginx/fcgiwrap.pid 6，修改nginx配置$vim /usr/local/nginx/conf/nginx.conf server { listen 8090;#设置监听端口 server_name localhost; error_log /usr/local/nginx/logs/cgi_test.log info; #设置日志路径和模式 location /hello.cgi { fastcgi_param SCRIPT_FILENAME /usr/local/nginx/cgi-bin/$fastcgi_script_name; #设置脚本存放目录 fastcgi_index index.cgi; include fastcgi_params; include fastcgi.conf; fastcgi_pass 127.0.0.1:8092; #监听端口 } } 7，nginx重启Nginx启动： /usr/local/nginx/sbin/nginx Nginx主进程号： ps -ef | grep &quot;nginx: master process&quot; | grep -v &quot;grep&quot; | awk -F &apos; &apos; &apos;{print $2}‘ cat /usr/local/nginx/logs/nginx.pid Nginx配置文件： /usr/local/nginx/conf/nginx.conf Nginx配置文件测试： sudo /usr/local/nginx/sbin/nginx –t 使配置生效： kill -HUP `cat /usr/local/nginx/logs/nginx.pid` 重启nginx后，在浏览器输入http://server_ip:port/hello.cgi 8，参考链接CGI相关：http://www.jdon.com/idea/cgi.htm http://www.cnblogs.com/skynet/p/4173450.html http://www.cnblogs.com/liuzhang/p/3929198.html nginx相关：http://developer.51cto.com/art/201004/194472.htm https://segmentfault.com/a/1190000002797601 http://blog.csdn.net/allenlinrui/article/details/19419721 http://blog.163.com/koumm@126/blog/static/9540383720096307529267/ http://zyan.cc/nginx_php_v5/","categories":[{"name":"后端开发","slug":"后端开发","permalink":"http://yoursite.com/categories/后端开发/"}],"tags":[{"name":"后端服务，CGI","slug":"后端服务，CGI","permalink":"http://yoursite.com/tags/后端服务，CGI/"}]},{"title":"R语言入门-数据读取","slug":"R语言入门-数据读取","date":"2016-05-11T13:00:49.000Z","updated":"2016-11-11T08:33:40.000Z","comments":true,"path":"2016/05/11/R语言入门-数据读取/","link":"","permalink":"http://yoursite.com/2016/05/11/R语言入门-数据读取/","excerpt":"","text":"1，首先需要确认工作空间getwd() 2，设置工作空间setwd(&apos;path&apos;) 3，确认当前路径下是否包含所需处理数据文件dir() 4，读取数据read.csv(&apos;csv_filename&apos;) read.table(&apos;filename&apos;,sep=&apos;delimeter&apos;，header=FALSE)#也可以用来读取csv文件，默认没有列名","categories":[{"name":"数据分析","slug":"数据分析","permalink":"http://yoursite.com/categories/数据分析/"}],"tags":[{"name":"R语言","slug":"R语言","permalink":"http://yoursite.com/tags/R语言/"}]},{"title":"R语言入门-尝试","slug":"R语言入门-尝试","date":"2016-05-11T07:59:38.000Z","updated":"2016-11-11T08:33:37.000Z","comments":true,"path":"2016/05/11/R语言入门-尝试/","link":"","permalink":"http://yoursite.com/2016/05/11/R语言入门-尝试/","excerpt":"","text":"突然对数据分析利器很感兴趣，就找同事追兵请教了一番。今天刚好整理下整个学习流程。 1，数据读取文本read.table() MySQLinstall.packages(&apos;RMySQL&apos;) ODPSRODPS 其他形式的数据读取（再补充）2，数据表现形式data.frame 3, 数据打理tidyr行变列，列变行（reshape2） dplyr分组汇总（plyr） 4，绘图ggplot2类似PS分图层绘图，图片 plotly动态的JavaScript显示，ggplotly（p） 5， 分享，输出RMarkdown输出为文档：pdf,word,html shiny输出交互类网页，接受用户输入（文本框，按钮，选择框等）。可用于定制分析系统。 6， 空间数据存储与可视化shape，leaflet 上述有很多包名的书写大小写有待考证，先上一部分，后续学习中完善。","categories":[{"name":"数据分析","slug":"数据分析","permalink":"http://yoursite.com/categories/数据分析/"}],"tags":[{"name":"R语言","slug":"R语言","permalink":"http://yoursite.com/tags/R语言/"}]}]}